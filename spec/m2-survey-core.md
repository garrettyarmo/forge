# Milestone 2: Survey Core Specification

> **Spec Version**: 1.0
> **Status**: Draft
> **Implements**: IMPLEMENTATION_PLAN.md § Milestone 2
> **Depends On**: [M1 Foundation](./m1-foundation.md)

---

## 1. Overview

### 1.1 Purpose

Implement the core survey functionality: configuration loading, GitHub integration, JavaScript/TypeScript parsing, and the `forge survey` command. This milestone delivers the ability to survey JavaScript-based repositories and produce a knowledge graph.

### 1.2 Key Principle: Deterministic Execution

**The survey phase is purely deterministic.** It uses tree-sitter AST parsing only - no LLM calls. This ensures:

- **Reproducibility**: Same input code always produces same graph output
- **Speed**: No API latency, no rate limits
- **Offline capability**: Works without network for local repos
- **Predictable costs**: Zero token usage during survey

LLMs are only used in the business context interview (Milestone 6), triggered explicitly via `--business-context` flag.

### 1.3 Success Criteria

1. `forge init` generates a valid, well-commented `forge.yaml`
2. `forge survey` successfully clones repos from a GitHub organization
3. `forge survey` works with local directory paths
4. JavaScript/TypeScript imports and AWS SDK calls are correctly detected
5. Graph is saved to the configured output path
6. Parser failures in one repo don't crash the entire survey
7. Survey completes within 60 seconds for a 10-repo organization

### 1.4 Non-Goals

- Python parsing (Milestone 3)
- Terraform parsing (Milestone 3)
- Implicit coupling detection (Milestone 4)
- Output serialization formats beyond JSON (Milestone 5)
- Business context interview (Milestone 6)

---

## 2. Configuration System

### 2.1 forge.yaml Schema

```yaml
# forge.yaml - Forge configuration file
# Generated by: forge init
# Documentation: https://forge.dev/docs/configuration

# ═══════════════════════════════════════════════════════════════════════════════
# REPOSITORY SOURCES
# ═══════════════════════════════════════════════════════════════════════════════
# Define where Forge should discover repositories. You can combine multiple sources.

repos:
  # Option 1: Discover all repos from a GitHub organization
  # Forge will use the GitHub API to list all repositories
  github_org: "my-company"

  # Option 2: Explicit list of GitHub repositories
  # Use this for selective surveying or when you don't want all org repos
  github_repos:
    - "my-company/api-gateway"
    - "my-company/user-service"
    - "my-company/payment-service"

  # Option 3: Local filesystem paths
  # Use for testing, air-gapped environments, or monorepos
  local_paths:
    - "/path/to/local/repo"
    - "./relative/path/to/repo"

  # Exclude patterns (applied to all sources)
  # Glob patterns matched against repo names
  exclude:
    - "*-deprecated"
    - "*-archive"
    - "*.old"
    - "fork-*"

# ═══════════════════════════════════════════════════════════════════════════════
# GITHUB CONFIGURATION
# ═══════════════════════════════════════════════════════════════════════════════

github:
  # Environment variable containing your GitHub Personal Access Token
  # The token needs 'repo' scope for private repos, or 'public_repo' for public only
  token_env: "GITHUB_TOKEN"

  # For GitHub Enterprise, set the API base URL
  # api_url: "https://github.mycompany.com/api/v3"

  # Clone method: "https" (default) or "ssh"
  clone_method: "https"

  # Number of concurrent clone operations (default: 4)
  clone_concurrency: 4

# ═══════════════════════════════════════════════════════════════════════════════
# LANGUAGE DETECTION
# ═══════════════════════════════════════════════════════════════════════════════
# Languages are AUTO-DETECTED from file extensions and config files.
# You do NOT need to configure this section for normal usage.
#
# Detection rules:
#   package.json        → JavaScript/TypeScript
#   requirements.txt    → Python
#   pyproject.toml      → Python
#   *.tf                → Terraform
#
# Only configure if you need to exclude specific languages:

languages:
  exclude: []
  # Example: exclude terraform parsing
  # exclude:
  #   - terraform

# ═══════════════════════════════════════════════════════════════════════════════
# OUTPUT CONFIGURATION
# ═══════════════════════════════════════════════════════════════════════════════

output:
  # Where to save the knowledge graph (relative or absolute path)
  graph_path: ".forge/graph.json"

  # Where to cache cloned repositories
  # Supports ~ for home directory
  cache_path: "~/.forge/repos"

# ═══════════════════════════════════════════════════════════════════════════════
# LLM CONFIGURATION (for business context interview)
# ═══════════════════════════════════════════════════════════════════════════════
# Only used when running: forge survey --business-context

llm:
  # LLM provider CLI to use: claude | gemini | codex
  provider: "claude"

  # Override CLI path if not in PATH
  # cli_path: "/usr/local/bin/claude"

# ═══════════════════════════════════════════════════════════════════════════════
# TOKEN BUDGET
# ═══════════════════════════════════════════════════════════════════════════════
# Default token budget for `forge map` output

token_budget: 8000
```

### 2.2 Configuration Data Structures

```rust
// forge-cli/src/config.rs

use serde::{Deserialize, Serialize};
use std::path::PathBuf;

/// Root configuration structure for forge.yaml
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ForgeConfig {
    /// Repository sources configuration
    pub repos: RepoConfig,

    /// GitHub-specific settings
    #[serde(default)]
    pub github: GitHubConfig,

    /// Language detection settings
    #[serde(default)]
    pub languages: LanguageConfig,

    /// Output paths configuration
    #[serde(default)]
    pub output: OutputConfig,

    /// LLM provider configuration (for business context)
    #[serde(default)]
    pub llm: LLMConfig,

    /// Default token budget for map output
    #[serde(default = "default_token_budget")]
    pub token_budget: u32,
}

fn default_token_budget() -> u32 {
    8000
}

/// Repository source configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RepoConfig {
    /// GitHub organization name (discovers all repos)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub github_org: Option<String>,

    /// Explicit list of GitHub repos (owner/repo format)
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub github_repos: Vec<String>,

    /// Local filesystem paths to repositories
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub local_paths: Vec<PathBuf>,

    /// Glob patterns to exclude repos by name
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub exclude: Vec<String>,
}

/// GitHub-specific configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GitHubConfig {
    /// Environment variable name containing the GitHub token
    #[serde(default = "default_token_env")]
    pub token_env: String,

    /// GitHub API base URL (for GitHub Enterprise)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub api_url: Option<String>,

    /// Clone method: "https" or "ssh"
    #[serde(default = "default_clone_method")]
    pub clone_method: CloneMethod,

    /// Number of concurrent clone operations
    #[serde(default = "default_clone_concurrency")]
    pub clone_concurrency: usize,
}

impl Default for GitHubConfig {
    fn default() -> Self {
        Self {
            token_env: default_token_env(),
            api_url: None,
            clone_method: default_clone_method(),
            clone_concurrency: default_clone_concurrency(),
        }
    }
}

fn default_token_env() -> String {
    "GITHUB_TOKEN".to_string()
}

fn default_clone_method() -> CloneMethod {
    CloneMethod::Https
}

fn default_clone_concurrency() -> usize {
    4
}

/// Git clone method
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "lowercase")]
pub enum CloneMethod {
    Https,
    Ssh,
}

/// Language detection configuration
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct LanguageConfig {
    /// Languages to exclude from detection
    #[serde(default)]
    pub exclude: Vec<String>,
}

/// Output configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OutputConfig {
    /// Path to save the knowledge graph
    #[serde(default = "default_graph_path")]
    pub graph_path: PathBuf,

    /// Path to cache cloned repositories
    #[serde(default = "default_cache_path")]
    pub cache_path: PathBuf,
}

impl Default for OutputConfig {
    fn default() -> Self {
        Self {
            graph_path: default_graph_path(),
            cache_path: default_cache_path(),
        }
    }
}

fn default_graph_path() -> PathBuf {
    PathBuf::from(".forge/graph.json")
}

fn default_cache_path() -> PathBuf {
    PathBuf::from("~/.forge/repos")
}

/// LLM provider configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LLMConfig {
    /// Provider name: claude, gemini, codex
    #[serde(default = "default_llm_provider")]
    pub provider: String,

    /// Custom CLI path
    #[serde(skip_serializing_if = "Option::is_none")]
    pub cli_path: Option<PathBuf>,
}

impl Default for LLMConfig {
    fn default() -> Self {
        Self {
            provider: default_llm_provider(),
            cli_path: None,
        }
    }
}

fn default_llm_provider() -> String {
    "claude".to_string()
}
```

### 2.3 Configuration Loading

```rust
// forge-cli/src/config.rs (continued)

use std::env;
use std::fs;
use std::path::Path;
use thiserror::Error;

#[derive(Debug, Error)]
pub enum ConfigError {
    #[error("Configuration file not found: {0}")]
    NotFound(PathBuf),

    #[error("Failed to read configuration file: {0}")]
    ReadError(#[from] std::io::Error),

    #[error("Failed to parse configuration: {0}")]
    ParseError(#[from] serde_yaml::Error),

    #[error("Invalid configuration: {0}")]
    ValidationError(String),

    #[error("Environment variable not set: {0}")]
    EnvVarMissing(String),
}

impl ForgeConfig {
    /// Load configuration from the default path (./forge.yaml)
    pub fn load_default() -> Result<Self, ConfigError> {
        Self::load_from_path(Path::new("forge.yaml"))
    }

    /// Load configuration from a specific path
    pub fn load_from_path(path: &Path) -> Result<Self, ConfigError> {
        if !path.exists() {
            return Err(ConfigError::NotFound(path.to_path_buf()));
        }

        let content = fs::read_to_string(path)?;
        let mut config: ForgeConfig = serde_yaml::from_str(&content)?;

        // Apply environment variable overrides
        config.apply_env_overrides();

        // Expand paths (~ -> home directory)
        config.expand_paths()?;

        // Validate configuration
        config.validate()?;

        Ok(config)
    }

    /// Apply environment variable overrides
    /// Variables follow pattern: FORGE_{SECTION}_{KEY}
    fn apply_env_overrides(&mut self) {
        // Override GitHub org
        if let Ok(org) = env::var("FORGE_REPOS_GITHUB_ORG") {
            self.repos.github_org = Some(org);
        }

        // Override output graph path
        if let Ok(path) = env::var("FORGE_OUTPUT_GRAPH_PATH") {
            self.output.graph_path = PathBuf::from(path);
        }

        // Override cache path
        if let Ok(path) = env::var("FORGE_OUTPUT_CACHE_PATH") {
            self.output.cache_path = PathBuf::from(path);
        }

        // Override token budget
        if let Ok(budget) = env::var("FORGE_TOKEN_BUDGET") {
            if let Ok(n) = budget.parse() {
                self.token_budget = n;
            }
        }
    }

    /// Expand ~ in paths to home directory
    fn expand_paths(&mut self) -> Result<(), ConfigError> {
        let home = dirs::home_dir()
            .ok_or_else(|| ConfigError::ValidationError("Cannot determine home directory".into()))?;

        // Expand cache_path
        if self.output.cache_path.starts_with("~") {
            let rest = self.output.cache_path.strip_prefix("~").unwrap();
            self.output.cache_path = home.join(rest);
        }

        // Expand graph_path if it uses ~
        if self.output.graph_path.starts_with("~") {
            let rest = self.output.graph_path.strip_prefix("~").unwrap();
            self.output.graph_path = home.join(rest);
        }

        Ok(())
    }

    /// Validate the configuration
    fn validate(&self) -> Result<(), ConfigError> {
        // Must have at least one repo source
        if self.repos.github_org.is_none()
            && self.repos.github_repos.is_empty()
            && self.repos.local_paths.is_empty()
        {
            return Err(ConfigError::ValidationError(
                "No repository sources configured. Set github_org, github_repos, or local_paths".into()
            ));
        }

        // Validate GitHub repos format
        for repo in &self.repos.github_repos {
            if !repo.contains('/') {
                return Err(ConfigError::ValidationError(
                    format!("Invalid repo format '{}'. Expected 'owner/repo'", repo)
                ));
            }
        }

        Ok(())
    }

    /// Get the GitHub token from the configured environment variable
    pub fn github_token(&self) -> Result<String, ConfigError> {
        env::var(&self.github.token_env)
            .map_err(|_| ConfigError::EnvVarMissing(self.github.token_env.clone()))
    }

    /// Check if GitHub token is available (without erroring)
    pub fn has_github_token(&self) -> bool {
        env::var(&self.github.token_env).is_ok()
    }
}
```

---

## 3. GitHub Integration

### 3.1 GitHub Client

```rust
// forge-survey/src/github.rs

use octocrab::Octocrab;
use std::path::{Path, PathBuf};
use tokio::process::Command;
use thiserror::Error;

#[derive(Debug, Error)]
pub enum GitHubError {
    #[error("GitHub API error: {0}")]
    ApiError(#[from] octocrab::Error),

    #[error("Git clone failed for {repo}: {message}")]
    CloneFailed { repo: String, message: String },

    #[error("Git pull failed for {repo}: {message}")]
    PullFailed { repo: String, message: String },

    #[error("Invalid repository format: {0}")]
    InvalidRepoFormat(String),

    #[error("Rate limited. Retry after {0} seconds")]
    RateLimited(u64),

    #[error("Repository not found: {0}")]
    RepoNotFound(String),

    #[error("Authentication failed. Check your GITHUB_TOKEN")]
    AuthFailed,
}

/// A repository discovered from GitHub
#[derive(Debug, Clone)]
pub struct RepoInfo {
    /// Full name: "owner/repo"
    pub full_name: String,

    /// Just the repo name
    pub name: String,

    /// Owner/organization
    pub owner: String,

    /// Clone URL (https or ssh)
    pub clone_url: String,

    /// Default branch
    pub default_branch: String,

    /// Primary language (as detected by GitHub)
    pub language: Option<String>,

    /// Whether repo is archived
    pub archived: bool,

    /// Whether repo is a fork
    pub fork: bool,

    /// Topics/tags
    pub topics: Vec<String>,
}

/// GitHub API client wrapper
pub struct GitHubClient {
    client: Octocrab,
    clone_method: CloneMethod,
}

#[derive(Debug, Clone, Copy)]
pub enum CloneMethod {
    Https,
    Ssh,
}

impl GitHubClient {
    /// Create a new client with a personal access token
    pub fn new(token: &str, api_url: Option<&str>, clone_method: CloneMethod) -> Result<Self, GitHubError> {
        let builder = Octocrab::builder().personal_token(token.to_string());

        let builder = if let Some(url) = api_url {
            builder.base_uri(url).map_err(|e| GitHubError::ApiError(e.into()))?
        } else {
            builder
        };

        let client = builder.build().map_err(|e| GitHubError::ApiError(e.into()))?;

        Ok(Self { client, clone_method })
    }

    /// List all repositories in an organization
    pub async fn list_org_repos(&self, org: &str) -> Result<Vec<RepoInfo>, GitHubError> {
        let mut repos = Vec::new();
        let mut page = 1u32;

        loop {
            let page_repos = self.client
                .orgs(org)
                .list_repos()
                .repo_type(octocrab::params::repos::Type::All)
                .sort(octocrab::params::repos::Sort::Updated)
                .per_page(100)
                .page(page)
                .send()
                .await?;

            if page_repos.items.is_empty() {
                break;
            }

            for repo in page_repos.items {
                repos.push(RepoInfo {
                    full_name: repo.full_name.clone().unwrap_or_default(),
                    name: repo.name.clone(),
                    owner: org.to_string(),
                    clone_url: self.get_clone_url(&repo),
                    default_branch: repo.default_branch.unwrap_or_else(|| "main".to_string()),
                    language: repo.language.map(|l| l.to_string()),
                    archived: repo.archived.unwrap_or(false),
                    fork: repo.fork.unwrap_or(false),
                    topics: repo.topics.unwrap_or_default(),
                });
            }

            page += 1;
        }

        Ok(repos)
    }

    /// Get information about a specific repository
    pub async fn get_repo(&self, owner: &str, repo: &str) -> Result<RepoInfo, GitHubError> {
        let repo_data = self.client
            .repos(owner, repo)
            .get()
            .await?;

        Ok(RepoInfo {
            full_name: repo_data.full_name.clone().unwrap_or_default(),
            name: repo_data.name.clone(),
            owner: owner.to_string(),
            clone_url: self.get_clone_url(&repo_data),
            default_branch: repo_data.default_branch.unwrap_or_else(|| "main".to_string()),
            language: repo_data.language.map(|l| l.to_string()),
            archived: repo_data.archived.unwrap_or(false),
            fork: repo_data.fork.unwrap_or(false),
            topics: repo_data.topics.unwrap_or_default(),
        })
    }

    fn get_clone_url(&self, repo: &octocrab::models::Repository) -> String {
        match self.clone_method {
            CloneMethod::Https => repo.clone_url.as_ref()
                .map(|u| u.to_string())
                .unwrap_or_default(),
            CloneMethod::Ssh => repo.ssh_url.clone().unwrap_or_default(),
        }
    }
}
```

### 3.2 Repository Cloning and Caching

```rust
// forge-survey/src/github.rs (continued)

/// Manages local repository cache
pub struct RepoCache {
    cache_dir: PathBuf,
    clone_method: CloneMethod,
}

impl RepoCache {
    pub fn new(cache_dir: PathBuf, clone_method: CloneMethod) -> Self {
        Self { cache_dir, clone_method }
    }

    /// Get the local path for a repository
    pub fn repo_path(&self, repo: &RepoInfo) -> PathBuf {
        self.cache_dir.join(&repo.owner).join(&repo.name)
    }

    /// Clone or update a repository
    pub async fn ensure_repo(&self, repo: &RepoInfo, token: Option<&str>) -> Result<PathBuf, GitHubError> {
        let local_path = self.repo_path(repo);

        if local_path.exists() {
            // Pull latest changes
            self.pull_repo(&local_path, repo).await?;
        } else {
            // Clone the repository
            self.clone_repo(repo, &local_path, token).await?;
        }

        Ok(local_path)
    }

    async fn clone_repo(
        &self,
        repo: &RepoInfo,
        local_path: &Path,
        token: Option<&str>,
    ) -> Result<(), GitHubError> {
        // Create parent directories
        if let Some(parent) = local_path.parent() {
            tokio::fs::create_dir_all(parent).await
                .map_err(|e| GitHubError::CloneFailed {
                    repo: repo.full_name.clone(),
                    message: format!("Failed to create directory: {}", e),
                })?;
        }

        // Build clone URL with token for HTTPS
        let clone_url = match (self.clone_method, token) {
            (CloneMethod::Https, Some(token)) => {
                // Insert token into URL: https://TOKEN@github.com/owner/repo.git
                repo.clone_url.replace("https://", &format!("https://{}@", token))
            }
            _ => repo.clone_url.clone(),
        };

        // Execute git clone
        let output = Command::new("git")
            .args([
                "clone",
                "--depth", "1",  // Shallow clone for speed
                "--single-branch",
                "--branch", &repo.default_branch,
                &clone_url,
                local_path.to_str().unwrap(),
            ])
            .output()
            .await
            .map_err(|e| GitHubError::CloneFailed {
                repo: repo.full_name.clone(),
                message: e.to_string(),
            })?;

        if !output.status.success() {
            let stderr = String::from_utf8_lossy(&output.stderr);
            return Err(GitHubError::CloneFailed {
                repo: repo.full_name.clone(),
                message: stderr.to_string(),
            });
        }

        Ok(())
    }

    async fn pull_repo(&self, local_path: &Path, repo: &RepoInfo) -> Result<(), GitHubError> {
        let output = Command::new("git")
            .args(["pull", "--ff-only"])
            .current_dir(local_path)
            .output()
            .await
            .map_err(|e| GitHubError::PullFailed {
                repo: repo.full_name.clone(),
                message: e.to_string(),
            })?;

        if !output.status.success() {
            // Pull failed, might be dirty or diverged - reset to origin
            let _ = Command::new("git")
                .args(["fetch", "origin"])
                .current_dir(local_path)
                .output()
                .await;

            let _ = Command::new("git")
                .args(["reset", "--hard", &format!("origin/{}", repo.default_branch)])
                .current_dir(local_path)
                .output()
                .await;
        }

        Ok(())
    }

    /// Get the current commit SHA of a repository
    pub async fn get_commit_sha(&self, local_path: &Path) -> Option<String> {
        let output = Command::new("git")
            .args(["rev-parse", "HEAD"])
            .current_dir(local_path)
            .output()
            .await
            .ok()?;

        if output.status.success() {
            Some(String::from_utf8_lossy(&output.stdout).trim().to_string())
        } else {
            None
        }
    }
}
```

---

## 4. Parser Architecture

### 4.1 Parser Trait

```rust
// forge-survey/src/parser/traits.rs

use std::path::Path;
use thiserror::Error;

/// Errors that can occur during parsing
#[derive(Debug, Error)]
pub enum ParserError {
    #[error("Failed to read file: {0}")]
    IoError(#[from] std::io::Error),

    #[error("Failed to parse file: {path}")]
    ParseFailed { path: String },

    #[error("Unsupported file type: {0}")]
    UnsupportedFileType(String),

    #[error("Tree-sitter error: {0}")]
    TreeSitterError(String),
}

/// A discovery made by a parser
#[derive(Debug, Clone)]
pub enum Discovery {
    /// A service entry point was found
    Service(ServiceDiscovery),

    /// An import/require statement was found
    Import(ImportDiscovery),

    /// An HTTP API call was detected
    ApiCall(ApiCallDiscovery),

    /// A database access was detected
    DatabaseAccess(DatabaseAccessDiscovery),

    /// A queue/message operation was detected
    QueueOperation(QueueOperationDiscovery),

    /// A cloud resource usage was detected
    CloudResourceUsage(CloudResourceDiscovery),
}

/// Details about a discovered service
#[derive(Debug, Clone)]
pub struct ServiceDiscovery {
    /// Service name (usually from package.json or directory name)
    pub name: String,

    /// Programming language
    pub language: String,

    /// Framework detected (express, fastify, flask, etc.)
    pub framework: Option<String>,

    /// Entry point file
    pub entry_point: String,

    /// Source file where detected
    pub source_file: String,

    /// Line number
    pub source_line: u32,
}

/// Details about an import statement
#[derive(Debug, Clone)]
pub struct ImportDiscovery {
    /// The module being imported
    pub module: String,

    /// Whether it's a relative import
    pub is_relative: bool,

    /// Specific items imported (if destructured)
    pub imported_items: Vec<String>,

    /// Source file
    pub source_file: String,

    /// Line number
    pub source_line: u32,
}

/// Details about an API call
#[derive(Debug, Clone)]
pub struct ApiCallDiscovery {
    /// The target URL or service
    pub target: String,

    /// HTTP method if known
    pub method: Option<String>,

    /// How it was detected (axios, fetch, etc.)
    pub detection_method: String,

    /// Source file
    pub source_file: String,

    /// Line number
    pub source_line: u32,
}

/// Details about database access
#[derive(Debug, Clone)]
pub struct DatabaseAccessDiscovery {
    /// Database type (dynamodb, postgresql, etc.)
    pub db_type: String,

    /// Table/collection name if known
    pub table_name: Option<String>,

    /// Operation type (read, write, both)
    pub operation: DatabaseOperation,

    /// How it was detected (boto3, aws-sdk, etc.)
    pub detection_method: String,

    /// Source file
    pub source_file: String,

    /// Line number
    pub source_line: u32,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum DatabaseOperation {
    Read,
    Write,
    ReadWrite,
    Unknown,
}

/// Details about queue/message operations
#[derive(Debug, Clone)]
pub struct QueueOperationDiscovery {
    /// Queue type (sqs, sns, eventbridge, etc.)
    pub queue_type: String,

    /// Queue/topic name or ARN if known
    pub queue_name: Option<String>,

    /// Operation (publish, subscribe)
    pub operation: QueueOperation,

    /// Source file
    pub source_file: String,

    /// Line number
    pub source_line: u32,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum QueueOperation {
    Publish,
    Subscribe,
    Unknown,
}

/// Details about cloud resource usage
#[derive(Debug, Clone)]
pub struct CloudResourceDiscovery {
    /// Resource type (s3, lambda, etc.)
    pub resource_type: String,

    /// Resource name or ARN if known
    pub resource_name: Option<String>,

    /// Source file
    pub source_file: String,

    /// Line number
    pub source_line: u32,
}

/// Trait for language parsers
pub trait Parser: Send + Sync {
    /// Returns the file extensions this parser handles
    fn supported_extensions(&self) -> &[&str];

    /// Parse a single file and return discoveries
    fn parse_file(&self, path: &Path, content: &str) -> Result<Vec<Discovery>, ParserError>;

    /// Parse an entire repository
    /// Default implementation walks the directory tree
    fn parse_repo(&self, repo_path: &Path) -> Result<Vec<Discovery>, ParserError> {
        let mut all_discoveries = Vec::new();
        let extensions = self.supported_extensions();

        for entry in walkdir::WalkDir::new(repo_path)
            .follow_links(true)
            .into_iter()
            .filter_entry(|e| !is_ignored_dir(e.file_name().to_str().unwrap_or("")))
        {
            let entry = match entry {
                Ok(e) => e,
                Err(_) => continue,
            };

            if !entry.file_type().is_file() {
                continue;
            }

            let path = entry.path();
            let ext = path.extension().and_then(|e| e.to_str()).unwrap_or("");

            if !extensions.contains(&ext) {
                continue;
            }

            let content = match std::fs::read_to_string(path) {
                Ok(c) => c,
                Err(_) => continue, // Skip unreadable files
            };

            match self.parse_file(path, &content) {
                Ok(discoveries) => all_discoveries.extend(discoveries),
                Err(e) => {
                    // Log but continue - don't fail entire survey for one file
                    tracing::warn!("Failed to parse {}: {}", path.display(), e);
                }
            }
        }

        Ok(all_discoveries)
    }
}

/// Directories to skip during parsing
fn is_ignored_dir(name: &str) -> bool {
    matches!(
        name,
        "node_modules"
            | ".git"
            | "dist"
            | "build"
            | "target"
            | ".next"
            | ".nuxt"
            | "coverage"
            | "__pycache__"
            | ".pytest_cache"
            | "venv"
            | ".venv"
            | "env"
            | ".env"
            | "vendor"
    )
}
```

### 4.2 JavaScript/TypeScript Parser

```rust
// forge-survey/src/parser/javascript.rs

use super::traits::*;
use std::path::Path;
use tree_sitter::{Parser as TSParser, Query, QueryCursor};

/// Parser for JavaScript and TypeScript files
pub struct JavaScriptParser {
    parser: TSParser,
    import_query: Query,
    aws_sdk_query: Query,
    http_client_query: Query,
    dynamodb_query: Query,
}

impl JavaScriptParser {
    pub fn new() -> Result<Self, ParserError> {
        let mut parser = TSParser::new();
        let language = tree_sitter_javascript::language();
        parser.set_language(&language)
            .map_err(|e| ParserError::TreeSitterError(e.to_string()))?;

        // Query for import statements
        // Matches: import X from 'Y', import { X } from 'Y', require('Y')
        let import_query = Query::new(
            &language,
            r#"
            (import_statement
              source: (string) @import_source)

            (call_expression
              function: (identifier) @func_name
              arguments: (arguments (string) @require_source)
              (#eq? @func_name "require"))
            "#,
        ).map_err(|e| ParserError::TreeSitterError(e.to_string()))?;

        // Query for AWS SDK usage
        let aws_sdk_query = Query::new(
            &language,
            r#"
            (import_statement
              source: (string) @source
              (#match? @source "(@aws-sdk|aws-sdk)"))

            (call_expression
              function: (identifier) @func
              arguments: (arguments (string) @source)
              (#eq? @func "require")
              (#match? @source "(@aws-sdk|aws-sdk)"))
            "#,
        ).map_err(|e| ParserError::TreeSitterError(e.to_string()))?;

        // Query for HTTP client usage (axios, fetch)
        let http_client_query = Query::new(
            &language,
            r#"
            (call_expression
              function: [
                (identifier) @func
                (member_expression
                  object: (identifier) @obj)
              ]
              (#match? @func "(fetch|axios)")
              (#match? @obj "axios"))
            "#,
        ).map_err(|e| ParserError::TreeSitterError(e.to_string()))?;

        // Query for DynamoDB operations
        let dynamodb_query = Query::new(
            &language,
            r#"
            (call_expression
              function: (member_expression
                object: (_) @client
                property: (property_identifier) @method)
              (#match? @method "(get|put|update|delete|query|scan|batchGet|batchWrite)"))
            "#,
        ).map_err(|e| ParserError::TreeSitterError(e.to_string()))?;

        Ok(Self {
            parser,
            import_query,
            aws_sdk_query,
            http_client_query,
            dynamodb_query,
        })
    }

    fn detect_imports(&self, tree: &tree_sitter::Tree, content: &str, path: &Path) -> Vec<Discovery> {
        let mut discoveries = Vec::new();
        let mut cursor = QueryCursor::new();

        for match_ in cursor.matches(&self.import_query, tree.root_node(), content.as_bytes()) {
            for capture in match_.captures {
                let node = capture.node;
                let text = &content[node.byte_range()];
                // Remove quotes from string
                let module = text.trim_matches(|c| c == '"' || c == '\'' || c == '`');

                discoveries.push(Discovery::Import(ImportDiscovery {
                    module: module.to_string(),
                    is_relative: module.starts_with('.'),
                    imported_items: self.extract_import_specifiers(tree, node, content),
                    source_file: path.to_string_lossy().to_string(),
                    source_line: node.start_position().row as u32 + 1,
                }));
            }
        }

        discoveries
    }

    /// Extract named import specifiers from an import statement.
    fn extract_import_specifiers(
        &self,
        _tree: &tree_sitter::Tree,
        source_node: Node,
        content: &str,
    ) -> Vec<String> {
        let mut items = Vec::new();

        // Walk up to the import_statement and find import_clause
        if let Some(import_stmt) = source_node.parent() {
            for i in 0..import_stmt.named_child_count() {
                if let Some(child) = import_stmt.named_child(i) {
                    if child.kind() == "import_clause" {
                        // Check for named_imports
                        for j in 0..child.named_child_count() {
                            if let Some(named) = child.named_child(j) {
                                if named.kind() == "named_imports" {
                                    // Extract import specifiers
                                    for k in 0..named.named_child_count() {
                                        if let Some(spec) = named.named_child(k) {
                                            if spec.kind() == "import_specifier" {
                                                if let Some(name) = spec.named_child(0) {
                                                    if let Ok(text) = name.utf8_text(content.as_bytes()) {
                                                        items.push(text.to_string());
                                                    }
                                                }
                                            }
                                        }
                                    }
                                } else if named.kind() == "identifier" {
                                    // Default import
                                    if let Ok(text) = named.utf8_text(content.as_bytes()) {
                                        items.push(text.to_string());
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }

        items
    }

    fn detect_aws_sdk(&self, tree: &tree_sitter::Tree, content: &str, path: &Path) -> Vec<Discovery> {
        let mut discoveries = Vec::new();
        let mut cursor = QueryCursor::new();

        for match_ in cursor.matches(&self.aws_sdk_query, tree.root_node(), content.as_bytes()) {
            for capture in match_.captures {
                let node = capture.node;
                let text = &content[node.byte_range()];

                // Determine which AWS service is being used
                let (resource_type, db_type) = if text.contains("dynamodb") || text.contains("DynamoDB") {
                    ("dynamodb", Some("dynamodb"))
                } else if text.contains("s3") || text.contains("S3") {
                    ("s3", None)
                } else if text.contains("sqs") || text.contains("SQS") {
                    ("sqs", None)
                } else if text.contains("sns") || text.contains("SNS") {
                    ("sns", None)
                } else if text.contains("lambda") || text.contains("Lambda") {
                    ("lambda", None)
                } else {
                    ("aws", None)
                };

                if let Some(db) = db_type {
                    discoveries.push(Discovery::DatabaseAccess(DatabaseAccessDiscovery {
                        db_type: db.to_string(),
                        table_name: None, // Will be extracted from DynamoDB calls
                        operation: DatabaseOperation::Unknown,
                        detection_method: "aws-sdk".to_string(),
                        source_file: path.to_string_lossy().to_string(),
                        source_line: node.start_position().row as u32 + 1,
                    }));
                } else if resource_type == "sqs" || resource_type == "sns" {
                    discoveries.push(Discovery::QueueOperation(QueueOperationDiscovery {
                        queue_type: resource_type.to_string(),
                        queue_name: None,
                        operation: QueueOperation::Unknown,
                        source_file: path.to_string_lossy().to_string(),
                        source_line: node.start_position().row as u32 + 1,
                    }));
                } else {
                    discoveries.push(Discovery::CloudResourceUsage(CloudResourceDiscovery {
                        resource_type: resource_type.to_string(),
                        resource_name: None,
                        source_file: path.to_string_lossy().to_string(),
                        source_line: node.start_position().row as u32 + 1,
                    }));
                }
            }
        }

        discoveries
    }

    fn detect_http_calls(&self, tree: &tree_sitter::Tree, content: &str, path: &Path) -> Vec<Discovery> {
        let mut discoveries = Vec::new();
        let mut cursor = QueryCursor::new();

        for match_ in cursor.matches(&self.http_client_query, tree.root_node(), content.as_bytes()) {
            for capture in match_.captures {
                let node = capture.node;
                let text = &content[node.byte_range()];

                // Try to extract URL from the call arguments
                let target = self.extract_url_from_call(&node, content)
                    .unwrap_or_else(|| "unknown".to_string());

                let method = if text.contains("get") || text.contains("GET") {
                    Some("GET".to_string())
                } else if text.contains("post") || text.contains("POST") {
                    Some("POST".to_string())
                } else if text.contains("put") || text.contains("PUT") {
                    Some("PUT".to_string())
                } else if text.contains("delete") || text.contains("DELETE") {
                    Some("DELETE".to_string())
                } else {
                    None
                };

                discoveries.push(Discovery::ApiCall(ApiCallDiscovery {
                    target,
                    method,
                    detection_method: if text.contains("axios") { "axios" } else { "fetch" }.to_string(),
                    source_file: path.to_string_lossy().to_string(),
                    source_line: node.start_position().row as u32 + 1,
                }));
            }
        }

        discoveries
    }

    fn detect_dynamodb_ops(&self, tree: &tree_sitter::Tree, content: &str, path: &Path) -> Vec<Discovery> {
        let mut discoveries = Vec::new();
        let mut cursor = QueryCursor::new();

        for match_ in cursor.matches(&self.dynamodb_query, tree.root_node(), content.as_bytes()) {
            for capture in match_.captures {
                if capture.index == 1 { // @method capture
                    let node = capture.node;
                    let method = &content[node.byte_range()];

                    let operation = match method {
                        "get" | "query" | "scan" | "batchGet" => DatabaseOperation::Read,
                        "put" | "delete" | "batchWrite" => DatabaseOperation::Write,
                        "update" => DatabaseOperation::ReadWrite,
                        _ => DatabaseOperation::Unknown,
                    };

                    // Try to extract table name from the call
                    let table_name = if let Some(parent) = node.parent() {
                        self.extract_table_name_from_call(&parent, content)
                    } else {
                        None
                    };

                    discoveries.push(Discovery::DatabaseAccess(DatabaseAccessDiscovery {
                        db_type: "dynamodb".to_string(),
                        table_name,
                        operation,
                        detection_method: "aws-sdk-method".to_string(),
                        source_file: path.to_string_lossy().to_string(),
                        source_line: node.start_position().row as u32 + 1,
                    }));
                }
            }
        }

        discoveries
    }

    /// Extract URL from HTTP call arguments.
    fn extract_url_from_call(&self, call_node: &Node, content: &str) -> Option<String> {
        if let Some(args) = call_node.child_by_field_name("arguments") {
            if let Some(first_arg) = args.named_child(0) {
                // Check if first argument is a string literal
                if first_arg.kind() == "string" {
                    let text = first_arg.utf8_text(content.as_bytes()).unwrap_or("");
                    return Some(
                        text.trim_matches(|c| c == '"' || c == '\'' || c == '`')
                            .to_string(),
                    );
                }
                // Check if it's a template literal
                if first_arg.kind() == "template_string" {
                    let text = first_arg.utf8_text(content.as_bytes()).unwrap_or("");
                    return Some(text.trim_matches('`').to_string());
                }
            }
        }
        None
    }

    /// Try to extract table name from a DynamoDB call.
    fn extract_table_name_from_call(&self, call_node: &Node, content: &str) -> Option<String> {
        // Look for TableName in the arguments
        if let Some(args) = call_node.child_by_field_name("arguments") {
            for i in 0..args.named_child_count() {
                if let Some(arg) = args.named_child(i) {
                    // Check if it's an object with TableName property
                    if arg.kind() == "object" {
                        return self.find_table_name_in_object(arg, content);
                    }
                }
            }
        }
        None
    }

    /// Find TableName property in an object literal.
    fn find_table_name_in_object(&self, obj_node: Node, content: &str) -> Option<String> {
        for i in 0..obj_node.named_child_count() {
            if let Some(child) = obj_node.named_child(i) {
                if child.kind() == "pair" {
                    if let Some(key) = child.child_by_field_name("key") {
                        let key_text = key.utf8_text(content.as_bytes()).unwrap_or("");
                        if key_text == "TableName" {
                            if let Some(value) = child.child_by_field_name("value") {
                                let value_text = value.utf8_text(content.as_bytes()).unwrap_or("");
                                return Some(
                                    value_text
                                        .trim_matches(|c| c == '"' || c == '\'' || c == '`')
                                        .to_string(),
                                );
                            }
                        }
                    }
                }
            }
        }
        None
    }
}

impl Parser for JavaScriptParser {
    fn supported_extensions(&self) -> &[&str] {
        &["js", "jsx", "ts", "tsx", "mjs", "cjs"]
    }

    fn parse_file(&self, path: &Path, content: &str) -> Result<Vec<Discovery>, ParserError> {
        // Parse the file
        let tree = self.parser.parse(content, None)
            .ok_or_else(|| ParserError::ParseFailed {
                path: path.to_string_lossy().to_string()
            })?;

        let mut discoveries = Vec::new();

        // Run all detectors
        discoveries.extend(self.detect_imports(&tree, content, path));
        discoveries.extend(self.detect_aws_sdk(&tree, content, path));
        discoveries.extend(self.detect_http_calls(&tree, content, path));
        discoveries.extend(self.detect_dynamodb_ops(&tree, content, path));

        Ok(discoveries)
    }
}
```

### 4.3 Service Detection from package.json

```rust
// forge-survey/src/parser/javascript.rs (continued)

impl JavaScriptParser {
    /// Parse package.json to detect service information
    pub fn parse_package_json(&self, repo_path: &Path) -> Option<ServiceDiscovery> {
        let package_json_path = repo_path.join("package.json");
        if !package_json_path.exists() {
            return None;
        }

        let content = std::fs::read_to_string(&package_json_path).ok()?;
        let package: serde_json::Value = serde_json::from_str(&content).ok()?;

        let name = package.get("name")?.as_str()?.to_string();

        // Detect framework from dependencies
        let framework = self.detect_framework(&package);

        // Find entry point
        let entry_point = package.get("main")
            .and_then(|m| m.as_str())
            .unwrap_or("index.js")
            .to_string();

        Some(ServiceDiscovery {
            name,
            language: "javascript".to_string(),
            framework,
            entry_point,
            source_file: package_json_path.to_string_lossy().to_string(),
            source_line: 1,
        })
    }

    fn detect_framework(&self, package: &serde_json::Value) -> Option<String> {
        let deps = package.get("dependencies")?;

        if deps.get("express").is_some() {
            Some("express".to_string())
        } else if deps.get("fastify").is_some() {
            Some("fastify".to_string())
        } else if deps.get("koa").is_some() {
            Some("koa".to_string())
        } else if deps.get("hapi").is_some() || deps.get("@hapi/hapi").is_some() {
            Some("hapi".to_string())
        } else if deps.get("next").is_some() {
            Some("next.js".to_string())
        } else if deps.get("nuxt").is_some() {
            Some("nuxt".to_string())
        } else if deps.get("@nestjs/core").is_some() {
            Some("nestjs".to_string())
        } else {
            None
        }
    }
}
```

---

## 5. Discovery to Graph Mapping

### 5.1 Graph Builder

```rust
// forge-survey/src/lib.rs

use forge_graph::{
    ForgeGraph, Node, NodeBuilder, NodeId, NodeType, Edge, EdgeType,
    DiscoverySource, AttributeValue,
};
use crate::parser::{Discovery, Parser, ServiceDiscovery};
use std::collections::HashMap;
use std::path::Path;

/// Builds a knowledge graph from parser discoveries
pub struct GraphBuilder {
    graph: ForgeGraph,
    /// Map from discovered service names to their NodeIds
    service_map: HashMap<String, NodeId>,
    /// Map from resource identifiers to NodeIds
    resource_map: HashMap<String, NodeId>,
    /// Current repo being processed
    current_repo: Option<String>,
    /// Current commit SHA
    current_commit: Option<String>,
}

impl GraphBuilder {
    pub fn new() -> Self {
        Self {
            graph: ForgeGraph::new(),
            service_map: HashMap::new(),
            resource_map: HashMap::new(),
            current_repo: None,
            current_commit: None,
        }
    }

    /// Load an existing graph to update
    pub fn from_graph(graph: ForgeGraph) -> Self {
        let mut builder = Self::new();
        builder.graph = graph;
        // Rebuild indexes from existing graph
        for node in builder.graph.nodes() {
            match node.node_type {
                NodeType::Service => {
                    builder.service_map.insert(node.display_name.clone(), node.id.clone());
                }
                NodeType::Database | NodeType::Queue | NodeType::CloudResource => {
                    builder.resource_map.insert(node.display_name.clone(), node.id.clone());
                }
                _ => {}
            }
        }
        builder
    }

    /// Set the current repo context
    pub fn set_repo_context(&mut self, repo_name: &str, commit_sha: Option<&str>) {
        self.current_repo = Some(repo_name.to_string());
        self.current_commit = commit_sha.map(|s| s.to_string());
    }

    /// Process a service discovery
    pub fn add_service(&mut self, discovery: ServiceDiscovery) -> NodeId {
        let namespace = self.current_repo.clone().unwrap_or_else(|| "unknown".to_string());
        let id = NodeId::new(NodeType::Service, &namespace, &discovery.name).unwrap();

        if self.service_map.contains_key(&discovery.name) {
            return self.service_map[&discovery.name].clone();
        }

        let node = NodeBuilder::new()
            .id(id.clone())
            .node_type(NodeType::Service)
            .display_name(&discovery.name)
            .attribute("language", discovery.language)
            .attribute("entry_point", discovery.entry_point)
            .attribute("repo_url", self.current_repo.clone().unwrap_or_default())
            .source(DiscoverySource::JavaScriptParser)
            .commit_sha_opt(self.current_commit.clone())
            .source_file(discovery.source_file)
            .source_line(discovery.source_line)
            .build()
            .unwrap();

        if let Some(framework) = discovery.framework {
            self.graph.get_node_mut(&id).map(|n| {
                n.attributes.insert("framework".to_string(), AttributeValue::String(framework));
            });
        }

        self.graph.upsert_node(node);
        self.service_map.insert(discovery.name, id.clone());
        id
    }

    /// Process all discoveries from a repository
    pub fn process_discoveries(&mut self, discoveries: Vec<Discovery>, service_id: &NodeId) {
        for discovery in discoveries {
            match discovery {
                Discovery::Service(svc) => {
                    self.add_service(svc);
                }
                Discovery::Import(import) => {
                    // Track imports for dependency analysis
                    // External imports might indicate service calls
                    if !import.is_relative && self.is_known_service(&import.module) {
                        self.add_service_call(service_id, &import.module, &import.source_file, import.source_line);
                    }
                }
                Discovery::ApiCall(call) => {
                    self.add_api_call(service_id, call);
                }
                Discovery::DatabaseAccess(db) => {
                    self.add_database_access(service_id, db);
                }
                Discovery::QueueOperation(queue) => {
                    self.add_queue_operation(service_id, queue);
                }
                Discovery::CloudResourceUsage(resource) => {
                    self.add_cloud_resource(service_id, resource);
                }
            }
        }
    }

    fn is_known_service(&self, module: &str) -> bool {
        // Check if this module name matches a known service
        self.service_map.contains_key(module)
    }

    fn add_service_call(&mut self, from: &NodeId, to_name: &str, source_file: &str, source_line: u32) {
        if let Some(to_id) = self.service_map.get(to_name) {
            let mut edge = Edge::new(from.clone(), to_id.clone(), EdgeType::Calls).unwrap();
            edge.metadata.evidence.push(format!("{}:{}", source_file, source_line));
            edge.metadata.discovered_at = chrono::Utc::now();
            let _ = self.graph.upsert_edge(edge);
        }
    }

    fn add_api_call(&mut self, service_id: &NodeId, call: crate::parser::ApiCallDiscovery) {
        // If we can identify the target service, create a CALLS edge
        // For now, just record the call in the service's attributes
        if let Some(node) = self.graph.get_node_mut(service_id) {
            let calls = node.attributes
                .entry("api_calls".to_string())
                .or_insert_with(|| AttributeValue::List(vec![]));

            if let AttributeValue::List(list) = calls {
                list.push(AttributeValue::Map(HashMap::from([
                    ("target".to_string(), AttributeValue::String(call.target)),
                    ("method".to_string(), AttributeValue::String(call.method.unwrap_or_default())),
                    ("source".to_string(), AttributeValue::String(format!("{}:{}", call.source_file, call.source_line))),
                ])));
            }
        }
    }

    fn add_database_access(&mut self, service_id: &NodeId, db: crate::parser::DatabaseAccessDiscovery) {
        // Create or get database node
        let db_name = db.table_name.clone().unwrap_or_else(|| format!("{}:{}", db.db_type, "unknown"));
        let namespace = self.current_repo.clone().unwrap_or_else(|| "unknown".to_string());

        let db_id = if let Some(id) = self.resource_map.get(&db_name) {
            id.clone()
        } else {
            let id = NodeId::new(NodeType::Database, &namespace, &db_name).unwrap();

            let node = NodeBuilder::new()
                .id(id.clone())
                .node_type(NodeType::Database)
                .display_name(&db_name)
                .attribute("db_type", db.db_type.clone())
                .source(DiscoverySource::JavaScriptParser)
                .source_file(db.source_file.clone())
                .source_line(db.source_line)
                .build()
                .unwrap();

            self.graph.upsert_node(node);
            self.resource_map.insert(db_name, id.clone());
            id
        };

        // Create edge based on operation type
        let edge_type = match db.operation {
            crate::parser::DatabaseOperation::Read => EdgeType::Reads,
            crate::parser::DatabaseOperation::Write => EdgeType::Writes,
            crate::parser::DatabaseOperation::ReadWrite => EdgeType::Reads, // Add both
            crate::parser::DatabaseOperation::Unknown => EdgeType::Reads, // Default to read
        };

        let mut edge = Edge::new(service_id.clone(), db_id.clone(), edge_type).unwrap();
        edge.metadata.evidence.push(format!("{}:{}", db.source_file, db.source_line));
        edge.metadata.discovered_at = chrono::Utc::now();
        let _ = self.graph.upsert_edge(edge);

        // If ReadWrite, add write edge too
        if db.operation == crate::parser::DatabaseOperation::ReadWrite {
            let mut write_edge = Edge::new(service_id.clone(), db_id, EdgeType::Writes).unwrap();
            write_edge.metadata.evidence.push(format!("{}:{}", db.source_file, db.source_line));
            write_edge.metadata.discovered_at = chrono::Utc::now();
            let _ = self.graph.upsert_edge(write_edge);
        }
    }

    fn add_queue_operation(&mut self, service_id: &NodeId, queue: crate::parser::QueueOperationDiscovery) {
        let queue_name = queue.queue_name.clone().unwrap_or_else(|| format!("{}:unknown", queue.queue_type));
        let namespace = self.current_repo.clone().unwrap_or_else(|| "unknown".to_string());

        let queue_id = if let Some(id) = self.resource_map.get(&queue_name) {
            id.clone()
        } else {
            let id = NodeId::new(NodeType::Queue, &namespace, &queue_name).unwrap();

            let node = NodeBuilder::new()
                .id(id.clone())
                .node_type(NodeType::Queue)
                .display_name(&queue_name)
                .attribute("queue_type", queue.queue_type.clone())
                .source(DiscoverySource::JavaScriptParser)
                .source_file(queue.source_file.clone())
                .source_line(queue.source_line)
                .build()
                .unwrap();

            self.graph.upsert_node(node);
            self.resource_map.insert(queue_name, id.clone());
            id
        };

        let edge_type = match queue.operation {
            crate::parser::QueueOperation::Publish => EdgeType::Publishes,
            crate::parser::QueueOperation::Subscribe => EdgeType::Subscribes,
            crate::parser::QueueOperation::Unknown => EdgeType::Uses,
        };

        let mut edge = Edge::new(service_id.clone(), queue_id, edge_type).unwrap();
        edge.metadata.evidence.push(format!("{}:{}", queue.source_file, queue.source_line));
        edge.metadata.discovered_at = chrono::Utc::now();
        let _ = self.graph.upsert_edge(edge);
    }

    fn add_cloud_resource(&mut self, service_id: &NodeId, resource: crate::parser::CloudResourceDiscovery) {
        let resource_name = resource.resource_name.clone().unwrap_or_else(|| format!("{}:unknown", resource.resource_type));
        let namespace = self.current_repo.clone().unwrap_or_else(|| "unknown".to_string());

        let resource_id = if let Some(id) = self.resource_map.get(&resource_name) {
            id.clone()
        } else {
            let id = NodeId::new(NodeType::CloudResource, &namespace, &resource_name).unwrap();

            let node = NodeBuilder::new()
                .id(id.clone())
                .node_type(NodeType::CloudResource)
                .display_name(&resource_name)
                .attribute("resource_type", resource.resource_type.clone())
                .source(DiscoverySource::JavaScriptParser)
                .source_file(resource.source_file.clone())
                .source_line(resource.source_line)
                .build()
                .unwrap();

            self.graph.upsert_node(node);
            self.resource_map.insert(resource_name, id.clone());
            id
        };

        let mut edge = Edge::new(service_id.clone(), resource_id, EdgeType::Uses).unwrap();
        edge.metadata.evidence.push(format!("{}:{}", resource.source_file, resource.source_line));
        edge.metadata.discovered_at = chrono::Utc::now();
        let _ = self.graph.upsert_edge(edge);
    }

    /// Get the built graph
    pub fn build(self) -> ForgeGraph {
        self.graph
    }
}
```

---

## 6. CLI Commands

### 6.1 forge init Command

```rust
// forge-cli/src/commands/init.rs

use crate::config::ForgeConfig;
use std::io::Write;
use std::path::Path;

const DEFAULT_CONFIG_TEMPLATE: &str = r#"# forge.yaml - Forge configuration file
# Documentation: https://forge.dev/docs/configuration

# ═══════════════════════════════════════════════════════════════════════════════
# REPOSITORY SOURCES
# ═══════════════════════════════════════════════════════════════════════════════

repos:
  # GitHub organization (discovers all repos)
  github_org: "{org}"

  # Or explicit repos:
  # github_repos:
  #   - "owner/repo-a"
  #   - "owner/repo-b"

  # Local paths for testing:
  # local_paths:
  #   - "./my-local-repo"

  # Exclude patterns:
  # exclude:
  #   - "*-deprecated"

# ═══════════════════════════════════════════════════════════════════════════════
# GITHUB CONFIGURATION
# ═══════════════════════════════════════════════════════════════════════════════

github:
  token_env: "GITHUB_TOKEN"
  # api_url: "https://github.mycompany.com/api/v3"  # For GitHub Enterprise
  clone_method: "https"

# ═══════════════════════════════════════════════════════════════════════════════
# OUTPUT CONFIGURATION
# ═══════════════════════════════════════════════════════════════════════════════

output:
  graph_path: ".forge/graph.json"
  cache_path: "~/.forge/repos"

# ═══════════════════════════════════════════════════════════════════════════════
# TOKEN BUDGET
# ═══════════════════════════════════════════════════════════════════════════════

token_budget: 8000
"#;

pub struct InitOptions {
    pub org: Option<String>,
    pub output: Option<String>,
    pub force: bool,
}

pub fn run_init(options: InitOptions) -> Result<(), Box<dyn std::error::Error>> {
    let output_path = options.output.unwrap_or_else(|| "forge.yaml".to_string());
    let path = Path::new(&output_path);

    // Check if file exists
    if path.exists() && !options.force {
        return Err(format!(
            "Configuration file already exists: {}. Use --force to overwrite.",
            output_path
        ).into());
    }

    // Generate config content
    let org = options.org.unwrap_or_else(|| "my-org".to_string());
    let content = DEFAULT_CONFIG_TEMPLATE.replace("{org}", &org);

    // Write file
    let mut file = std::fs::File::create(path)?;
    file.write_all(content.as_bytes())?;

    println!("Created configuration file: {}", output_path);
    println!();
    println!("Next steps:");
    println!("  1. Edit {} to configure your repositories", output_path);
    println!("  2. Set your GitHub token: export GITHUB_TOKEN=<your-token>");
    println!("  3. Run: forge survey");

    Ok(())
}
```

### 6.2 forge survey Command

```rust
// forge-cli/src/commands/survey.rs

use crate::config::ForgeConfig;
use forge_survey::{
    github::{GitHubClient, RepoCache, RepoInfo},
    GraphBuilder,
    parser::javascript::JavaScriptParser,
};
use std::path::PathBuf;
use indicatif::{ProgressBar, ProgressStyle};

pub struct SurveyOptions {
    pub config_path: Option<String>,
    pub output_path: Option<String>,
    pub repos: Option<Vec<String>>,
    pub exclude_langs: Option<Vec<String>>,
    pub incremental: bool,
    pub verbose: bool,
}

pub async fn run_survey(options: SurveyOptions) -> Result<(), Box<dyn std::error::Error>> {
    // Load configuration
    let config = if let Some(path) = &options.config_path {
        ForgeConfig::load_from_path(std::path::Path::new(path))?
    } else {
        ForgeConfig::load_default()?
    };

    // Determine output path
    let output_path = options.output_path
        .map(PathBuf::from)
        .unwrap_or_else(|| config.output.graph_path.clone());

    // Collect repositories to survey
    let repos = collect_repos(&config, options.repos).await?;

    if repos.is_empty() {
        println!("No repositories found to survey.");
        return Ok(());
    }

    println!("Found {} repositories to survey", repos.len());

    // Initialize graph builder
    let mut builder = if options.incremental && output_path.exists() {
        let graph = forge_graph::ForgeGraph::load_from_file(&output_path)?;
        GraphBuilder::from_graph(graph)
    } else {
        GraphBuilder::new()
    };

    // Set up repo cache
    let cache = RepoCache::new(
        config.output.cache_path.clone(),
        match config.github.clone_method {
            crate::config::CloneMethod::Https => forge_survey::github::CloneMethod::Https,
            crate::config::CloneMethod::Ssh => forge_survey::github::CloneMethod::Ssh,
        },
    );

    // Get GitHub token if available
    let token = config.github_token().ok();

    // Set up progress bar
    let progress = ProgressBar::new(repos.len() as u64);
    progress.set_style(
        ProgressStyle::default_bar()
            .template("{spinner:.green} [{bar:40.cyan/blue}] {pos}/{len} {msg}")
            .unwrap()
            .progress_chars("#>-")
    );

    // Process each repository
    let js_parser = JavaScriptParser::new()?;

    for repo in repos {
        progress.set_message(format!("Surveying {}", repo.full_name));

        // Clone or update repository
        let local_path = match cache.ensure_repo(&repo, token.as_deref()).await {
            Ok(path) => path,
            Err(e) => {
                if options.verbose {
                    eprintln!("Warning: Failed to clone {}: {}", repo.full_name, e);
                }
                progress.inc(1);
                continue;
            }
        };

        // Get commit SHA
        let commit_sha = cache.get_commit_sha(&local_path).await;

        // Set repo context
        builder.set_repo_context(&repo.full_name, commit_sha.as_deref());

        // Detect service from package.json
        if let Some(service) = js_parser.parse_package_json(&local_path) {
            let service_id = builder.add_service(service);

            // Parse all JS/TS files
            match js_parser.parse_repo(&local_path) {
                Ok(discoveries) => {
                    builder.process_discoveries(discoveries, &service_id);
                }
                Err(e) => {
                    if options.verbose {
                        eprintln!("Warning: Failed to parse {}: {}", repo.full_name, e);
                    }
                }
            }
        }

        progress.inc(1);
    }

    progress.finish_with_message("Survey complete");

    // Save graph
    let graph = builder.build();

    // Create output directory if needed
    if let Some(parent) = output_path.parent() {
        std::fs::create_dir_all(parent)?;
    }

    graph.save_to_file(&output_path)?;

    println!();
    println!("Knowledge graph saved to: {}", output_path.display());
    println!("  Nodes: {}", graph.node_count());
    println!("  Edges: {}", graph.edge_count());

    Ok(())
}

async fn collect_repos(
    config: &ForgeConfig,
    override_repos: Option<Vec<String>>,
) -> Result<Vec<RepoInfo>, Box<dyn std::error::Error>> {
    let mut repos = Vec::new();

    // If repos are overridden via CLI, use those
    if let Some(repo_list) = override_repos {
        for repo_str in repo_list {
            let parts: Vec<&str> = repo_str.split('/').collect();
            if parts.len() != 2 {
                return Err(format!("Invalid repo format: {}. Expected owner/repo", repo_str).into());
            }

            repos.push(RepoInfo {
                full_name: repo_str.clone(),
                name: parts[1].to_string(),
                owner: parts[0].to_string(),
                clone_url: format!("https://github.com/{}.git", repo_str),
                default_branch: "main".to_string(),
                language: None,
                archived: false,
                fork: false,
                topics: vec![],
            });
        }
        return Ok(repos);
    }

    // Collect from GitHub org
    if let Some(org) = &config.repos.github_org {
        let token = config.github_token()?;
        let client = GitHubClient::new(
            &token,
            config.github.api_url.as_deref(),
            match config.github.clone_method {
                crate::config::CloneMethod::Https => forge_survey::github::CloneMethod::Https,
                crate::config::CloneMethod::Ssh => forge_survey::github::CloneMethod::Ssh,
            },
        )?;

        let org_repos = client.list_org_repos(org).await?;
        repos.extend(org_repos);
    }

    // Collect explicit GitHub repos
    if !config.repos.github_repos.is_empty() {
        let token = config.github_token()?;
        let client = GitHubClient::new(
            &token,
            config.github.api_url.as_deref(),
            match config.github.clone_method {
                crate::config::CloneMethod::Https => forge_survey::github::CloneMethod::Https,
                crate::config::CloneMethod::Ssh => forge_survey::github::CloneMethod::Ssh,
            },
        )?;

        for repo_str in &config.repos.github_repos {
            let parts: Vec<&str> = repo_str.split('/').collect();
            if parts.len() == 2 {
                match client.get_repo(parts[0], parts[1]).await {
                    Ok(repo) => repos.push(repo),
                    Err(e) => eprintln!("Warning: Could not fetch {}: {}", repo_str, e),
                }
            }
        }
    }

    // Collect local paths
    for local_path in &config.repos.local_paths {
        let path = if local_path.is_relative() {
            std::env::current_dir()?.join(local_path)
        } else {
            local_path.clone()
        };

        if path.exists() {
            let name = path.file_name()
                .and_then(|n| n.to_str())
                .unwrap_or("unknown")
                .to_string();

            repos.push(RepoInfo {
                full_name: format!("local/{}", name),
                name: name.clone(),
                owner: "local".to_string(),
                clone_url: path.to_string_lossy().to_string(),
                default_branch: "main".to_string(),
                language: None,
                archived: false,
                fork: false,
                topics: vec![],
            });
        }
    }

    // Apply exclusion patterns
    repos = repos.into_iter()
        .filter(|r| !r.archived)  // Exclude archived by default
        .filter(|r| !is_excluded(&r.name, &config.repos.exclude))
        .collect();

    Ok(repos)
}

fn is_excluded(name: &str, patterns: &[String]) -> bool {
    for pattern in patterns {
        if glob::Pattern::new(pattern)
            .map(|p| p.matches(name))
            .unwrap_or(false)
        {
            return true;
        }
    }
    false
}
```

---

## 7. Test Specifications

### 7.1 Configuration Tests

```rust
#[cfg(test)]
mod config_tests {
    use super::*;
    use tempfile::tempdir;

    #[test]
    fn test_load_minimal_config() {
        let yaml = r#"
repos:
  github_org: "test-org"
"#;
        let dir = tempdir().unwrap();
        let path = dir.path().join("forge.yaml");
        std::fs::write(&path, yaml).unwrap();

        let config = ForgeConfig::load_from_path(&path).unwrap();
        assert_eq!(config.repos.github_org, Some("test-org".to_string()));
    }

    #[test]
    fn test_config_validation_no_repos() {
        let yaml = r#"
repos: {}
"#;
        let dir = tempdir().unwrap();
        let path = dir.path().join("forge.yaml");
        std::fs::write(&path, yaml).unwrap();

        let result = ForgeConfig::load_from_path(&path);
        assert!(matches!(result, Err(ConfigError::ValidationError(_))));
    }

    #[test]
    fn test_env_override() {
        std::env::set_var("FORGE_REPOS_GITHUB_ORG", "env-org");

        let yaml = r#"
repos:
  github_org: "yaml-org"
"#;
        let dir = tempdir().unwrap();
        let path = dir.path().join("forge.yaml");
        std::fs::write(&path, yaml).unwrap();

        let config = ForgeConfig::load_from_path(&path).unwrap();
        assert_eq!(config.repos.github_org, Some("env-org".to_string()));

        std::env::remove_var("FORGE_REPOS_GITHUB_ORG");
    }
}
```

### 7.2 JavaScript Parser Tests

```rust
#[cfg(test)]
mod javascript_parser_tests {
    use super::*;

    #[test]
    fn test_detect_es6_imports() {
        let parser = JavaScriptParser::new().unwrap();
        let content = r#"
import express from 'express';
import { DynamoDB } from '@aws-sdk/client-dynamodb';
import axios from 'axios';
"#;

        let discoveries = parser.parse_file(Path::new("test.js"), content).unwrap();

        let imports: Vec<_> = discoveries.iter()
            .filter_map(|d| match d {
                Discovery::Import(i) => Some(i),
                _ => None,
            })
            .collect();

        assert_eq!(imports.len(), 3);
        assert!(imports.iter().any(|i| i.module == "express"));
        assert!(imports.iter().any(|i| i.module == "@aws-sdk/client-dynamodb"));
        assert!(imports.iter().any(|i| i.module == "axios"));
    }

    #[test]
    fn test_detect_require_statements() {
        let parser = JavaScriptParser::new().unwrap();
        let content = r#"
const express = require('express');
const AWS = require('aws-sdk');
"#;

        let discoveries = parser.parse_file(Path::new("test.js"), content).unwrap();

        let imports: Vec<_> = discoveries.iter()
            .filter_map(|d| match d {
                Discovery::Import(i) => Some(i),
                _ => None,
            })
            .collect();

        assert!(imports.iter().any(|i| i.module == "express"));
        assert!(imports.iter().any(|i| i.module == "aws-sdk"));
    }

    #[test]
    fn test_detect_aws_sdk_v3() {
        let parser = JavaScriptParser::new().unwrap();
        let content = r#"
import { DynamoDBClient } from '@aws-sdk/client-dynamodb';
import { S3Client } from '@aws-sdk/client-s3';
import { SQSClient } from '@aws-sdk/client-sqs';
"#;

        let discoveries = parser.parse_file(Path::new("test.js"), content).unwrap();

        let db_accesses: Vec<_> = discoveries.iter()
            .filter_map(|d| match d {
                Discovery::DatabaseAccess(db) => Some(db),
                _ => None,
            })
            .collect();

        assert!(db_accesses.iter().any(|d| d.db_type == "dynamodb"));
    }

    #[test]
    fn test_detect_dynamodb_operations() {
        let parser = JavaScriptParser::new().unwrap();
        let content = r#"
const result = await docClient.get({ TableName: 'users', Key: { id } });
await docClient.put({ TableName: 'users', Item: user });
const items = await docClient.query({ TableName: 'orders' });
"#;

        let discoveries = parser.parse_file(Path::new("test.js"), content).unwrap();

        let db_ops: Vec<_> = discoveries.iter()
            .filter_map(|d| match d {
                Discovery::DatabaseAccess(db) => Some(db),
                _ => None,
            })
            .collect();

        // Should detect get, put, query operations
        assert!(db_ops.iter().any(|d| d.operation == DatabaseOperation::Read));
        assert!(db_ops.iter().any(|d| d.operation == DatabaseOperation::Write));
    }

    #[test]
    fn test_package_json_parsing() {
        let parser = JavaScriptParser::new().unwrap();
        let dir = tempdir().unwrap();

        let package_json = r#"
{
  "name": "user-service",
  "main": "dist/index.js",
  "dependencies": {
    "express": "^4.18.0",
    "@aws-sdk/client-dynamodb": "^3.0.0"
  }
}
"#;
        std::fs::write(dir.path().join("package.json"), package_json).unwrap();

        let service = parser.parse_package_json(dir.path()).unwrap();

        assert_eq!(service.name, "user-service");
        assert_eq!(service.framework, Some("express".to_string()));
        assert_eq!(service.entry_point, "dist/index.js");
    }
}
```

### 7.3 Integration Tests

```rust
// forge-survey/tests/integration_js.rs

use tempfile::tempdir;
use std::fs;

#[tokio::test]
async fn test_survey_synthetic_js_repo() {
    let dir = tempdir().unwrap();
    let repo_path = dir.path().join("test-service");
    fs::create_dir_all(&repo_path).unwrap();

    // Create package.json
    fs::write(
        repo_path.join("package.json"),
        r#"{"name": "test-service", "dependencies": {"express": "4.18.0", "@aws-sdk/client-dynamodb": "3.0.0"}}"#
    ).unwrap();

    // Create source files
    fs::create_dir_all(repo_path.join("src")).unwrap();
    fs::write(
        repo_path.join("src/index.js"),
        r#"
import express from 'express';
import { DynamoDBClient, GetCommand } from '@aws-sdk/client-dynamodb';

const app = express();
const client = new DynamoDBClient({});

app.get('/users/:id', async (req, res) => {
    const result = await client.send(new GetCommand({
        TableName: 'users',
        Key: { id: req.params.id }
    }));
    res.json(result.Item);
});
"#
    ).unwrap();

    // Run survey
    let parser = JavaScriptParser::new().unwrap();
    let mut builder = GraphBuilder::new();

    if let Some(service) = parser.parse_package_json(&repo_path) {
        let service_id = builder.add_service(service);
        let discoveries = parser.parse_repo(&repo_path).unwrap();
        builder.process_discoveries(discoveries, &service_id);
    }

    let graph = builder.build();

    // Verify results
    assert!(graph.node_count() > 0);
    assert!(graph.nodes_by_type(NodeType::Service).count() >= 1);

    // Should have detected DynamoDB access
    let has_db = graph.nodes_by_type(NodeType::Database).count() > 0
        || graph.edges().any(|e| matches!(e.edge_type, EdgeType::Reads | EdgeType::Writes));
    assert!(has_db, "Should detect database access");
}
```

---

## 8. Implementation Checklist

| Task ID | Description | Files |
|---------|-------------|-------|
| M2-T1 | Define forge.yaml schema | `forge-cli/src/config.rs` |
| M2-T2 | Implement config loading | `forge-cli/src/config.rs` |
| M2-T3 | Implement `forge init` | `forge-cli/src/commands/init.rs` |
| M2-T4 | Implement GitHub API client | `forge-survey/src/github.rs` |
| M2-T5 | Define Parser trait | `forge-survey/src/parser/traits.rs` |
| M2-T6 | Implement JS/TS parser | `forge-survey/src/parser/javascript.rs` |
| M2-T7 | Implement discovery-to-graph mapper | `forge-survey/src/lib.rs` |
| M2-T8 | Implement `forge survey` command | `forge-cli/src/commands/survey.rs` |
| M2-T9 | Write JS parser unit tests | `forge-survey/src/parser/javascript.rs` |
| M2-T10 | Write integration tests | `forge-survey/tests/integration_js.rs` |

---

## 9. Acceptance Criteria

- [ ] `forge init` creates a valid, well-documented `forge.yaml`
- [ ] `forge init --org my-org` pre-fills the organization name
- [ ] `forge survey` clones repos from a GitHub organization
- [ ] `forge survey` works with `local_paths` configuration
- [ ] ES6 imports (`import X from 'Y'`) are detected
- [ ] CommonJS requires (`require('Y')`) are detected
- [ ] AWS SDK v2 (`aws-sdk`) imports are detected
- [ ] AWS SDK v3 (`@aws-sdk/*`) imports are detected
- [ ] DynamoDB operations (get, put, query, scan) are detected
- [ ] Graph is saved to the configured `output.graph_path`
- [ ] Parser failures in one repo don't crash the entire survey
- [ ] Exclusion patterns work (`*-deprecated`, etc.)
- [ ] Progress is displayed during survey
- [ ] Survey completes within reasonable time (< 60s for 10 repos)
